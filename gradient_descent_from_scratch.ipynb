{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an exercise to understand the Logistic Regression algorithm used in machine learning for supervised classification tasks. \n",
    "\n",
    "An overview of the mathematics used can be found on Andrew Ng's Coursera course for deep learning. This exercise borrows heavily, although not exclusively, from that. The dataset we will be working with today is the titanic survival dataset found on Kaggle where passengers are labeled 1 if they survived and 0 if they did not and we are asked to predict which passangers lived given a dataset without survival labels. I chose this data because everyone has heard of the data set and I had easy access to it. The data is really not important for this exercise, as long as it is generic enough to generalize to similar use cases. \n",
    "\n",
    "### Important:\n",
    "The goal here is to understand the algorithm rather than showcase the Data Science and exploration processes that occur both before and after the point at which we decide on and build an algorithm. There are many steps left out (on purpuse) like data exploration, model and cross validation, feature selection and much more. \n",
    "\n",
    "Lets first take a look at a standard implimentation from Sklearn that is prepackaged, then build out the algorithm the long way (notably inefficient, then we will use Linear Algebra to improve efficiency for scalability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Logistic Regression using Sklearn (Prepackaged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df1 = pd.read_csv('titanic_train.csv')\n",
    "df2 = pd.read_csv('titanic_test.csv')\n",
    "\n",
    "# do same preprocessing on fare\n",
    "df1.Fare.fillna(14.45, inplace=True)\n",
    "df2.Fare.fillna(14.45, inplace=True)\n",
    "df1['fare'] = df1['Fare']/513\n",
    "df2['fare'] = df2['Fare']/513\n",
    "\n",
    "# first class indicator\n",
    "df1['first_class'] = (df1['Pclass'] == 1).astype(int)\n",
    "df2['first_class'] = (df2['Pclass'] == 1).astype(int)\n",
    "\n",
    "# female indicator\n",
    "df1['female_flag'] = (df1['Sex'] == 'female').astype(int)\n",
    "df2['female_flag'] = (df2['Sex'] == 'female').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=333, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df1.loc[:, ['fare', 'first_class', 'female_flag']]\n",
    "y_train = df1['Survived']\n",
    "\n",
    "model = LogisticRegression(random_state=333, solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression from Scratch (First Way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "\n",
    "def import_data(pathname, train_or_test):\n",
    "    with open(pathname, newline='') as csvfile:\n",
    "        filereader = csv.reader(csvfile, delimiter=',')\n",
    "    \n",
    "        survived = list()\n",
    "        fare = list()\n",
    "        first_class = list()\n",
    "        female_flag = list()\n",
    "\n",
    "        for idx, row in enumerate(filereader):\n",
    "            if idx == 0:\n",
    "                fare_ix = row.index('Fare')\n",
    "                fc_ix = row.index('Pclass')\n",
    "                sex_ix = row.index('Sex')\n",
    "                if train_or_test == 'train':\n",
    "                    surv_ix = row.index('Survived')\n",
    "            if idx != 0:\n",
    "                if train_or_test == 'train':\n",
    "                    survived.append(int(row[surv_ix]))\n",
    "                else:\n",
    "                    survived.append('?')\n",
    "                first_class.append(int(row[fc_ix]=='1'))\n",
    "                try:\n",
    "                    fare.append(float(row[fare_ix]))\n",
    "                except:\n",
    "                    fare.append(14.45) #training data median fare\n",
    "                female_flag.append(int(row[sex_ix]=='female'))\n",
    "\n",
    "                \n",
    "    return survived, fare, first_class, female_flag\n",
    "\n",
    "def sigmoid_function(x):\n",
    "    return 1/(1 + math.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "survived_tr, fare_tr, first_class_tr, female_flag_tr = import_data('titanic_train.csv', 'train')\n",
    "survived_tst, fare_tst, first_class_tst, female_flag_tst = import_data('titanic_test.csv', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0  loss:  0.693 || w1=-0.001, w2=-0.004, w3=-0.005, b=-0.015\n",
      "iteration:  1000  loss:  1.776 || w1=-0.193, w2=-0.675, w3=-0.945, b=-3.651\n",
      "iteration:  2000  loss:  2.083 || w1=-0.224, w2=-0.762, w3=-1.059, b=-4.358\n",
      "iteration:  3000  loss:  2.26 || w1=-0.241, w2=-0.809, w3=-1.12, b=-4.765\n",
      "iteration:  4000  loss:  2.384 || w1=-0.253, w2=-0.841, w3=-1.16, b=-5.052\n",
      "iteration:  5000  loss:  2.48 || w1=-0.262, w2=-0.865, w3=-1.19, b=-5.273\n",
      "iteration:  6000  loss:  2.558 || w1=-0.269, w2=-0.883, w3=-1.215, b=-5.454\n",
      "iteration:  7000  loss:  2.624 || w1=-0.275, w2=-0.899, w3=-1.234, b=-5.606\n",
      "iteration:  8000  loss:  2.681 || w1=-0.28, w2=-0.913, w3=-1.251, b=-5.738\n",
      "iteration:  9000  loss:  2.731 || w1=-0.285, w2=-0.924, w3=-1.266, b=-5.854\n",
      "iteration:  10000  loss:  2.776 || w1=-0.289, w2=-0.935, w3=-1.279, b=-5.958\n"
     ]
    }
   ],
   "source": [
    "# train logistic regression model\n",
    "\n",
    "w1, w2, w3, b = 0, 0, 0, 0 #initialize\n",
    "learning_rate = .06\n",
    "\n",
    "for k in range(10_001):\n",
    "    J, dw1, dw2, dw3, db = 0, 0, 0, 0, 0 #initialize \n",
    "    for i in range(len(survived_tr)):\n",
    "        y  = survived_tr[i]\n",
    "        z  = (w1*(fare_tr[i]/513) + #min max scale fare using train max\n",
    "              w2*first_class_tr[i] + \n",
    "              w3*female_flag_tr[i] + \n",
    "              b)\n",
    "        a  = sigmoid_function(z)\n",
    "        dz = a - y\n",
    "        J   += -(y*math.log(a) + (1-y)*math.log(1-a))\n",
    "        dw1 += (fare_tr[i]/513)*dz \n",
    "        dw2 += first_class_tr[i]*dz\n",
    "        dw3 += female_flag_tr[i]*dz\n",
    "        db  += dz\n",
    "\n",
    "    dw1 /= len(survived_tr)\n",
    "    dw2 /= len(survived_tr)\n",
    "    dw3 /= len(survived_tr)\n",
    "    db  /= len(survived_tr)\n",
    "    J   /= len(survived_tr)\n",
    "\n",
    "    w1 -= learning_rate*(dw1)\n",
    "    w2 -= learning_rate*(dw2)\n",
    "    w3 -= learning_rate*(dw3)\n",
    "    b  -= learning_rate*(db)\n",
    "    \n",
    "    if k%1_000 == 0:\n",
    "        print(\n",
    "            'iteration: ',k,\n",
    "            ' loss: ',round(J,3),\n",
    "            '|| w1={}, w2={}, w3={}, b={}'\n",
    "                .format(round(w1,3),round(w2,3),round(w3,3),round(b,3))\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression vectorized (second way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#required imports\n",
    "# import pandas as pd\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(x_dim):\n",
    "    weights = np.zeros((x_dim, 1))\n",
    "    b = 0.0\n",
    "    return weights, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_np(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(w, b, X, Y):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    Y = np.array(Y).reshape(1,Y.shape[0])\n",
    "    A = sigmoid_np(np.dot(w.T, X.T) + b)\n",
    "    cost = (-1/m)*np.sum(Y*np.log(A) + (1-Y)*np.log(1-A))\n",
    "    \n",
    "    dw = (1/m)*(np.dot(X.T, (A-Y).T))\n",
    "    db = (1/m)*np.sum(A-Y)\n",
    "    \n",
    "    return cost, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(w, b, X, Y, steps, alpha):\n",
    "    cost_curve = list()\n",
    "    \n",
    "    for i in range(steps+1):\n",
    "        \n",
    "        cost, dw, db = forward_pass(w, b, X, Y)\n",
    "        \n",
    "        w -= alpha*dw\n",
    "        b -= alpha*db\n",
    "        \n",
    "        if i % 2_000 == 0:\n",
    "            cost_curve.append(cost)\n",
    "            print('Cost at iteration {}: {}'.format(i, cost))\n",
    "            \n",
    "    return w, b, cost_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, Y_train, steps=10_000, alpha=.06):\n",
    "    \n",
    "    #initialize\n",
    "    weights, b = initialize_weights(X_train.shape[1])\n",
    "    \n",
    "    #optimize\n",
    "    w, b, cost_curve = gradient_descent(\n",
    "        weights, b, X_train, Y_train, steps=steps, alpha=alpha)\n",
    "    \n",
    "    return w, b, cost_curve\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration 0: 0.6931471805599454\n",
      "Cost at iteration 2000: 0.4773103297648902\n",
      "Cost at iteration 4000: 0.47687641624497756\n",
      "Cost at iteration 6000: 0.47680513332820845\n",
      "Cost at iteration 8000: 0.47675104507479743\n",
      "Cost at iteration 10000: 0.47670837546033246\n"
     ]
    }
   ],
   "source": [
    "vect_w, vect_b, cost_curve = train_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(w, b, X):\n",
    "#     m = X.shape[0]\n",
    "    \n",
    "#     Y_preds = np.zeros((1, m))\n",
    "#     w = w.reshape((X.shape[1], 1))\n",
    "    \n",
    "#     pred = sigmoid_np(np.dot(w.T, X.T) + b)\n",
    "    \n",
    "#     for i in range(pred.shape[1]):\n",
    "#         Y_preds[0,i] = int(pred[0][i] > 0.5)\n",
    "        \n",
    "#     return Y_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare</th>\n",
       "      <th>first_class</th>\n",
       "      <th>female_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.014133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.138954</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.015448</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fare  first_class  female_flag\n",
       "0  0.014133            0            0\n",
       "1  0.138954            1            1\n",
       "2  0.015448            0            1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how closely the different methodologies converged to a solution. w1 is the weight associated with fare or ticket price. w2 is the weight for the indicator variable if a passanger was in a first class cabin. w3 is the weight for the indicator if a passanger on the titanic was a female. b is the intercept (bias) term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1:  0.6759643072666862\n",
      "w2:  1.3928607842089231\n",
      "w3:  2.4987085910576283\n",
      "b:  -1.8429052148114269\n"
     ]
    }
   ],
   "source": [
    "print('w1: ', model.coef_[0][0])\n",
    "print('w2: ', model.coef_[0][1])\n",
    "print('w3: ', model.coef_[0][2])\n",
    "print('b: ', model.intercept_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression from scratch using loops (first way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1:  0.6778369918314936\n",
      "w2:  1.4953049780875323\n",
      "w3:  2.6177557917653225\n",
      "b:  -1.9325952928155834\n"
     ]
    }
   ],
   "source": [
    "print('w1: ', w1)\n",
    "print('w2: ', w2)\n",
    "print('w3: ', w3)\n",
    "print('b: ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression from scratch vectorized (second way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1:  0.6778369918314934\n",
      "w1:  1.4953049780875325\n",
      "w1:  2.6177557917653225\n",
      "b:  -1.9325952928155836\n"
     ]
    }
   ],
   "source": [
    "print('w1: ', vect_w[0][0])\n",
    "print('w1: ', vect_w[1][0])\n",
    "print('w1: ', vect_w[2][0])\n",
    "print('b: ', vect_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implimentations from scratch match exactly and show two different ways of arriving at the same answer. The weights the algorithm optimized on are very similar to the Sklearn implimentation of Logistic Regression but slightly different because they use different stopping criteria and different optimization parameters. Extremely close for all intents and purposes. \n",
    "\n",
    "In practice we often just impliment the the standard Instantiate, fit, predict pipeline for machine learning without too much thought about what goes inside that black box. Running through exercises like this not only allows you to understand how you can modify algorithms as we get more advanced but also will allow us to explain how we are arriving at solutions to all audiences from completely novice to extremely technical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add gradient_descent_from_scratch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git commit -m \"finished vectorization and added more comment and results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git push -u origin master"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
